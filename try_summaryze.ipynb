{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314da460-ae45-4f6a-a119-4f9a125f40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e08ccbc-ce91-42a2-a4b2-868e74627be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annot.opcorpora.xml  dict.opcorpora.xml  \u001b[0m\u001b[01;34mtask1\u001b[0m/\n",
      "\u001b[01;34marchive\u001b[0m/             requirements.txt    \u001b[01;34mtask2\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36d06e8d-0ebe-4e58-9a0a-cac787d5bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../archive/cnn_dailymail/train.csv\")\n",
    "test_df = pd.read_csv(\"../archive/cnn_dailymail/test.csv\")\n",
    "validation_df = pd.read_csv(\"../archive/cnn_dailymail/validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b354641-d003-4281-bbd7-0824b0600ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              id  \\\n",
      "count                                     287113   \n",
      "unique                                    287113   \n",
      "top     0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
      "freq                                           1   \n",
      "\n",
      "                                                  article  \\\n",
      "count                                              287113   \n",
      "unique                                             284005   \n",
      "top     (CNN) -- Dubai could lose its place on the Wom...   \n",
      "freq                                                    3   \n",
      "\n",
      "                                               highlights  \n",
      "count                                              287113  \n",
      "unique                                             282197  \n",
      "top     This page includes the show Transcript and the...  \n",
      "freq                                                   83  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80542c3-404a-46d0-b9fd-e0ea141ea9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Preprocessing (text-based EDA)\n",
    "# Remove any non-alphanumeric characters and extra whitespaces\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "train_df['article'] = train_df['article'].apply(clean_text)\n",
    "train_df['highlights'] = train_df['highlights'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f5b293-0b41-4a5f-81ef-163646d1d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ffeb52e-f76b-4b0c-9df1-60c7189b300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate summaries\n",
    "def generate_summary(article_text):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + article_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=100, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f66d64-a27b-447c-80e6-a8ab7e421dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to calculate ROUGE scores\n",
    "def calculate_rouge_scores(original_summary, generated_summary):  \n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'])\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    rouge_scores = scorer.score(generated_summary, original_summary)\n",
    "    return rouge_scores\n",
    "\n",
    "# def calculate_rouge_scores(original_summary, generated_summary):\n",
    "#     rouge = load_metric(\"rouge\")\n",
    "#     scores = rouge.compute(predictions=[generated_summary], references=[original_summary])\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913eebc5-26bd-46a6-b022-0d9676215c38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bart open-LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "918b5cef-740c-47ad-8900-82e23def0373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f270cebb3d4af48715382c071b165a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ab7c1af4134ecfa50bc7ad4fc8c5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6d791e0c5f4e089071eb04b4f4b1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7c81878b304f3faada6db350cdef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77132e3b76a4da69fee51cf02e5e63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b924ede4d654143a854582d45da20f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the BART model name\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "# Load the BART tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62654f45-9b34-4a62-ab0a-b7af126feeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43964/1083119517.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n",
      "/home/kefear/Documents/maga3sem/NLP/.venv/lib/python3.11/site-packages/datasets/load.py:752: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a5508d9d0345869bfdca8f5b5525d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Summary:\n",
      "Experts question if  packed out planes are putting passengers at risk .\n",
      "U.S consumer advisory group says minimum space must be stipulated .\n",
      "Safety tests conducted on planes with more leg room than airlines offer .\n",
      "\n",
      "Generated Summary:\n",
      "U.S consumer advisory group set up by Department of Transportation said that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased.\n"
     ]
    }
   ],
   "source": [
    "# Summarize and evaluate a single article from the test dataset\n",
    "article = test_df.iloc[0]['article']\n",
    "original_summary = test_df.iloc[0]['highlights']\n",
    "generated_summary = generate_summary(article)\n",
    "rouge_scores = calculate_rouge_scores(original_summary, generated_summary)\n",
    "\n",
    "print(\"Original Summary:\")\n",
    "print(original_summary)\n",
    "print(\"\\nGenerated Summary:\")\n",
    "print(generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a627d38-0e01-4460-995e-9e95a3e6e7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:\n",
      "Precision: 0.4117647058823529\n",
      "Recall: 0.24561403508771928\n",
      "F1 Score: 0.3076923076923077\n",
      "\n",
      "rouge2:\n",
      "Precision: 0.21212121212121213\n",
      "Recall: 0.125\n",
      "F1 Score: 0.15730337078651685\n",
      "\n",
      "rougeL:\n",
      "Precision: 0.35294117647058826\n",
      "Recall: 0.21052631578947367\n",
      "F1 Score: 0.2637362637362637\n",
      "\n",
      "rougeLsum:\n",
      "Precision: 0.38235294117647056\n",
      "Recall: 0.22807017543859648\n",
      "F1 Score: 0.28571428571428564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print ROUGE scores line by line\n",
    "for metric, scores in rouge_scores.items():\n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"Precision: {scores.precision}\")\n",
    "    print(f\"Recall: {scores.recall}\")\n",
    "    print(f\"F1 Score: {scores.fmeasure}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03566590-2cfe-4920-b8bf-272dc71dfda2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## T5 (Text-to-Text Transfer Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "795b88a7-593a-45f7-893b-a3a3709488cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "075cec9d-6752-479a-b0dd-cc0c83ef3702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d82a9e033249c68d6a7b7c514ae047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6ca4403fb14ec893268c8de95bbb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cae27cbfc34ce1b58e092b797351f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d2e2ca29934f6fa089a4f3d30ddd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67540fb5e3bc48038238790b0bc2791e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff9cea45f6c497f9ba8785c59765cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the T5 model and tokenizer\n",
    "model_name=\"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef275aa0-9f0d-4316-92a0-ffa67be41862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Summary:\n",
      "Experts question if  packed out planes are putting passengers at risk .\n",
      "U.S consumer advisory group says minimum space must be stipulated .\n",
      "Safety tests conducted on planes with more leg room than airlines offer .\n",
      "\n",
      "Generated Summary:\n",
      "some experts are questioning if shrinking space on planes is putting our health and safety in danger. this week, a consumer advisory group set up by the department of transportation said that while the government is happy to set standards for animals flying on planes, it does not stipulate a minimum amount of space for humans.\n"
     ]
    }
   ],
   "source": [
    "# Summarize and evaluate a single article from the test dataset\n",
    "article = test_df.iloc[0]['article']\n",
    "original_summary = test_df.iloc[0]['highlights']\n",
    "generated_summary = generate_summary(article)\n",
    "rouge_scores = calculate_rouge_scores(original_summary, generated_summary)\n",
    "\n",
    "print(\"Original Summary:\")\n",
    "print(original_summary)\n",
    "print(\"\\nGenerated Summary:\")\n",
    "print(generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7d0abbc-7003-428e-b8a7-5ad3e01f6b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:\n",
      "Precision: 0.38235294117647056\n",
      "Recall: 0.23214285714285715\n",
      "F1 Score: 0.28888888888888886\n",
      "\n",
      "rouge2:\n",
      "Precision: 0.09090909090909091\n",
      "Recall: 0.05454545454545454\n",
      "F1 Score: 0.06818181818181819\n",
      "\n",
      "rougeL:\n",
      "Precision: 0.2647058823529412\n",
      "Recall: 0.16071428571428573\n",
      "F1 Score: 0.19999999999999998\n",
      "\n",
      "rougeLsum:\n",
      "Precision: 0.35294117647058826\n",
      "Recall: 0.21428571428571427\n",
      "F1 Score: 0.26666666666666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print ROUGE scores line by line\n",
    "for metric, scores in rouge_scores.items():\n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"Precision: {scores.precision}\")\n",
    "    print(f\"Recall: {scores.recall}\")\n",
    "    print(f\"F1 Score: {scores.fmeasure}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691cd48-2741-453e-a027-e2c7fdafa78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð½Ð° ÑÐ°Ð¼Ð¾Ð¼ Ð´ÐµÐ»Ðµ ÑÑ‚Ð° Ð°Ð½Ð½Ð¾Ñ‚Ð°Ñ†Ð¸Ñ Ð¼Ð½Ðµ Ð½Ñ€Ð°Ð²Ð¸Ñ‚ÑÑ Ð±Ð¾Ð»ÑŒÑˆÐµ Ñ‡ÐµÐ¼ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð°Ñ,\n",
    "# Ñ…Ð¾Ñ‚Ñ Ð¿Ð¾ ÑÐºÐ¾Ñ€Ñƒ ÐºÐ°Ðº Ð±ÑƒÐ´Ñ‚Ð¾ Ñ…ÑƒÐ¶Ðµ, Ð½Ð¾ Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ ÑÐ¼Ñ‹ÑÐ» Ð¿Ð»ÑŽÑ Ð¼Ð¸Ð½ÑƒÑ ÐºÐ°Ðº Ð² Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»Ðµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c57a344-461b-4546-8762-f5ddd0522d22",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2bc91b2-8f51-462b-a163-fd6eb03c6c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "127a611f-1438-4ff7-a7d4-a12d4bc5176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "802eb63b-9357-46ae-a83b-154979cf9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "def annotate_text_with_distilbert(input_text, num_sentences=3):\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Get the hidden states from the DistilBERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    \n",
    "    # Extract the embeddings for each token\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    # Calculate the importance scores for each token (e.g., sum of token embeddings along the sequence dimension)\n",
    "    importance_scores = torch.sum(embeddings, dim=2).squeeze()\n",
    "    \n",
    "    # Calculate the sum of importance scores for each sentence\n",
    "    sentence_importance = torch.sum(importance_scores, dim=0)\n",
    "    \n",
    "    if sentence_importance.dim() == 0:\n",
    "        # If sentence_importance is a scalar, convert it to a tensor with one dimension\n",
    "        sentence_importance = sentence_importance.unsqueeze(0)\n",
    "    \n",
    "    # Get the top N sentences based on the sum of importance scores\n",
    "    top_indices = torch.topk(sentence_importance, k=min(num_sentences, sentence_importance.size(0)), dim=0).indices\n",
    "    \n",
    "    # Sort the indices to maintain order\n",
    "    top_indices = sorted(top_indices.tolist())\n",
    "    \n",
    "    # Decode the tokens to get the original text\n",
    "    decoded_text = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "    \n",
    "    # Split the text into sentences\n",
    "    sentences = decoded_text.split('. ')\n",
    "    \n",
    "    # Extract the top N sentences from the decoded text\n",
    "    annotated_text = '. '.join([sentences[i] for i in top_indices if i < len(sentences)])\n",
    "    \n",
    "    return annotated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f3979f6-173d-43ff-b73b-8d23e5d6c967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Summary:\n",
      "Experts question if  packed out planes are putting passengers at risk .\n",
      "U.S consumer advisory group says minimum space must be stipulated .\n",
      "Safety tests conducted on planes with more leg room than airlines offer .\n",
      "\n",
      "Generated Summary:\n",
      "ever noticed how plane seats appear to be getting smaller and smaller? with increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on a sample article\n",
    "sample_index = 0  # Change this to evaluate other articles\n",
    "article = test_df.iloc[sample_index]['article']\n",
    "original_summary = test_df.iloc[sample_index]['highlights']\n",
    "generated_summary = annotate_text_with_distilbert(article)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = calculate_rouge_scores(original_summary, generated_summary)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Summary:\")\n",
    "print(original_summary)\n",
    "print(\"\\nGenerated Summary:\")\n",
    "print(generated_summary)\n",
    "\n",
    "# Debugging output for generated_summary\n",
    "# print(\"\\nGenerated Summary (Debug Output):\")\n",
    "# print(repr(generated_summary))  # This will print the repr of the string, including special characters\n",
    "# Check if generated_summary is empty\n",
    "# if not generated_summary.strip():\n",
    "#     print(\"\\nWARNING: generated_summary is empty!\")\n",
    "# Print the original code for generated_summary\n",
    "# print(\"\\nGenerated Summary (Original):\")\n",
    "# print(generated_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc2941de-9775-4c19-9b2e-b6c4e2bd0682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1:\n",
      "Precision: 0.35294117647058826\n",
      "Recall: 0.3333333333333333\n",
      "F1 Score: 0.34285714285714286\n",
      "\n",
      "rouge2:\n",
      "Precision: 0.15151515151515152\n",
      "Recall: 0.14285714285714285\n",
      "F1 Score: 0.14705882352941174\n",
      "\n",
      "rougeL:\n",
      "Precision: 0.2647058823529412\n",
      "Recall: 0.25\n",
      "F1 Score: 0.2571428571428572\n",
      "\n",
      "rougeLsum:\n",
      "Precision: 0.3235294117647059\n",
      "Recall: 0.3055555555555556\n",
      "F1 Score: 0.31428571428571433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print ROUGE scores line by line\n",
    "for metric, scores in rouge_scores.items():\n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"Precision: {scores.precision}\")\n",
    "    print(f\"Recall: {scores.recall}\")\n",
    "    print(f\"F1 Score: {scores.fmeasure}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
